{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "Due Dec 14\n",
    "\n",
    "PCA Lesson: https://online.stat.psu.edu/stat505/lesson/11\n",
    "\n",
    "* Data: Boyer, Rick, and David Savageau. Places rated almanac: Your guide to finding the best places to live in America. Rand McNally & Company, 1985. \n",
    "* Data: http://www.stat.nthu.edu.tw/~swcheng/Teaching/stat5191/assignment/places.txt\n",
    "\n",
    "1) Normalize the data (apply log, subtract mean, normalize std dev)\n",
    "\n",
    "2) Perform PCA (use SVD or the eigenvalue decomposition of the covariance matrix)\n",
    "\n",
    "3) Plot the Scree plot. How much variance is explained by the first three PCs?\n",
    "\n",
    "4) Scatter plot all communities along two of the PCs (PC0 vs PC1 or PC1 vs PC2)\n",
    "\n",
    "5) Scatter plot all original dimensions in the space of PC0 and PC1.\n",
    "\n",
    "6) Do something interesting as you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "# standard libs\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import seaborn as sns #visualisation\n",
    "import statistics \n",
    "\n",
    "#ML with sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.cm import register_cmap\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA as PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dat\n",
    "df = pd.read_csv('places_updated.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(\"DATA INFO: \",np.shape(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.figure(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Climate_and_Terrain','Housing','Health_Care_and_Environment', 'Crime','Transportation','Education','Recreation','Economics']\n",
    "df[columns].plot.area()\n",
    "plt.figure(figsize=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('State').mean().plot.bar()\n",
    "plt.xticks(rotation=85)\n",
    "plt.figure(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel density function\n",
    "df.plot.kde(subplots=True, figsize=(5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Housing','Climate_and_Terrain','Health_Care_and_Environment', 'Crime','Transportation','Education','Recreation','Economics']\n",
    "df[columns].plot.box()\n",
    "plt.xticks(rotation='vertical')\n",
    "#For Housing and Crime, the lower the score the better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) with Python\n",
    "#### Inspired by District Data Labs \n",
    "https://medium.com/district-data-labs/principal-component-analysis-with-python-4962cd026465 \n",
    "\n",
    "#### Step 1: Load and Standardize Data\n",
    "First weâ€™ll load the data and store it in a pandas dataframe. The data set contains 9 categories (instances) for 329 cities (features). We need to make sure that we standardize the data by transforming it onto a unit scale (mean=0 and variance=1). Also, all null (NaN) values needs to converted to 0. It is necessary to transform data because PCA can only be applied on numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import*\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import register_cmap\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import seaborn\n",
    "\n",
    "#Load movie names and movie ratings\n",
    "# import cities\n",
    "places = pd.read_csv('places.csv')\n",
    "places.shape\n",
    "places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Place_ID','Housing','Climate_and_Terrain','Health_Care_and_Environment', 'Crime','Transportation','Education','The_Arts','Recreation','Economics']\n",
    "df1 = places[columns]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.replace(np.nan, 0, regex=True)\n",
    "X_std = StandardScaler().fit_transform(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Covariance Matrix and Eigendecomposition\n",
    "Next, a covariance matrix is created based on the standardized data. The covariance matrix is a representation of the covariance between each feature in the original dataset.\n",
    "The covariance matrix can be found as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vec = np.mean(X_std, axis=0)\n",
    "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
    "print('Covariance matrix \\n%s' %cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the covariance matrix is generated, eigendecomposition is performed on the covariance matrix. Eigenvectors and eigenvalues are found as a result of the eigendceomposition. Each eigenvector has a corresponding eigenvalue, and the sum of the eigenvalues represents all of the variance within the entire dataset.\n",
    "The eigendecomposition can be performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform eigendecomposition on covariance matrix\n",
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Selecting Principal Components\n",
    "Eigenvectors, or principal components, are a normalized linear combination of the features in the original dataset. The first principal component captures the most variance in the original variables, and the second component is a representation of the second highest variance within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVD decomposition to implement PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(df1)\n",
    "print (pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output tells me that 75% of the dataset's variance lies along the first PC, 14% along second, and 5% along the third one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance\n",
    "pca = PCA().fit(X_std)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Visualization in Python\n",
    "\n",
    "Visualize Principle Component Analysis (PCA) of your high-dimensional data in Python with Plotly.\n",
    "\n",
    "https://plotly.com/python/pca-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize all the original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cities\n",
    "df = pd.read_csv('places_updated.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df = df\n",
    "features = ['Climate_and_Terrain','Education','Economics']\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    df,\n",
    "    dimensions=features,\n",
    "    color=\"State\"\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D PCA Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = df\n",
    "X = places[['Climate_and_Terrain','Health_Care_and_Environment','Transportation','Education','The_Arts','Recreation','Economics']]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "fig = px.scatter(components, x=0, y=1, color=df['City'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = df\n",
    "X = places[['Climate_and_Terrain','Health_Care_and_Environment','Transportation','Education','The_Arts','Recreation','Economics']]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=df['City'],\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=df['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "c1 = components[:,1]\n",
    "c2 = components[:,2]\n",
    "ax.scatter(c1, c2)\n",
    "for name, x, y in zip(state, c1, c2):\n",
    "    ax.annotate(name, (x, y))\n",
    "ax.grid(True)\n",
    "ax.set_frame_on(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(X)\n",
    "housing= df['Housing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as deco\n",
    "variance_explained = 0.95\n",
    "pca = deco.PCA(variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(housing, components)\n",
    "ax.legend((\"PC1\", \"PC2\", \"PC3\"))\n",
    "ax.grid(True)\n",
    "ax.set_frame_on(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do something interesting as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HousingCost by state\n",
    "from collections import defaultdict\n",
    "state_HousingCost = defaultdict(int)\n",
    "for s, x in zip(df['State'], np.array(df['Housing'])):\n",
    "    state_HousingCost[s] += x\n",
    "print(state_HousingCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_by_sate = df.groupby('State').mean()\n",
    "df_mean_by_sate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std by column\n",
    "df_sdt_by_state = df.groupby('State').std()\n",
    "df_sdt_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std by column\n",
    "df_std = np.std(df)\n",
    "print(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean by column\n",
    "df_mean = np.mean(df)\n",
    "print(df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale all values in the Housing and Crime columns:\n",
    "\n",
    "scale = StandardScaler()\n",
    "\n",
    "X = df[['Housing', 'Crime']]\n",
    "\n",
    "scaledX = scale.fit_transform(X)\n",
    "\n",
    "print(scaledX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "#X has been defined in previous run now I need to define y, what I want to predic on the bais of Crime and Housing\n",
    "# I choose Education\n",
    "y = df['Education']\n",
    "\n",
    "#linear model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(scaledX, y)\n",
    "\n",
    "#what will be the education for housing at 8k and crime at 500?\n",
    "scaled = scale.transform([[8000, 500]])\n",
    "\n",
    "predicted_edu = regr.predict([scaled[0]])\n",
    "print(\"Education will be:\",predicted_edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing a scatter plot for Housing and Crime\n",
    "x = df['Housing']\n",
    "y = df['Crime']\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"scatter plot for Housing and Crime\")\n",
    "plt.show()\n",
    "\n",
    "#draw the line of Polynomial Regression for Housing and Crime\n",
    "\n",
    "mymodel = np.poly1d(np.polyfit(x, y, 4))\n",
    "\n",
    "myline = np.linspace(5159, 23640, 2498, 308)\n",
    "\n",
    "plt.scatter(x, y, c='grey')\n",
    "plt.plot(myline, mymodel(myline), color='r')\n",
    "plt.title(\"Polynomial Regression for Housing and Crime\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing a scatter plot for Housing and Crime\n",
    "x = df['Housing']\n",
    "y = df['Education']\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"scatter plot for Housing and Education\")\n",
    "plt.show()\n",
    "\n",
    "mymodel = np.poly1d(np.polyfit(x, y, 4))\n",
    "\n",
    "myline = np.linspace(5159, 23640, 3781)\n",
    "\n",
    "plt.scatter(x, y, c='grey')\n",
    "plt.plot(myline, mymodel(myline), color='r')\n",
    "plt.title(\"Polynomial Regression for Housing and Education\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Into Train/Test\n",
    "#The training set should be a random selection of 80% of the original data.\n",
    "#The testing set should be the remaining 20%.\n",
    "x = df['Housing']\n",
    "y = df['Crime']\n",
    "\n",
    "train_x = x[:80]\n",
    "train_y = y[:80]\n",
    "test_x = x[80:]\n",
    "test_y = y[80:]\n",
    "\n",
    "plt.scatter(train_x, train_y)\n",
    "plt.title(\"Trainning Data for Housing and Crime\")\n",
    "plt.show()\n",
    "\n",
    "#How well does my training data fit in a polynomial regression?\n",
    "np.random.seed(2)\n",
    "\n",
    "x = np.random.normal(x)\n",
    "y = np.random.normal(y) / x\n",
    "\n",
    "train_x = x[:80]\n",
    "train_y = y[:80]\n",
    "\n",
    "test_x = x[80:]\n",
    "test_y = y[80:]\n",
    "\n",
    "mymodel = np.poly1d(np.polyfit(train_x, train_y, 4))\n",
    "\n",
    "myline = np.linspace(23640, 3781, 5159)\n",
    "\n",
    "plt.scatter(train_x, train_y, c=\"grey\")\n",
    "plt.plot(myline, mymodel(myline),color=\"red\")\n",
    "plt.title(\"Training data fit in a polynomial regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 â€“ Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA using SVD decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_centered, U.dot(S).dot(Vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D_using_svd = X2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 3 principal comonents for time:\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,4))\n",
    "ax.plot(t, (U.T @ X)[:3, :].T)\n",
    "ax.grid(True)\n",
    "ax.set_frame_on(False)\n",
    "ax.legend((\"PC0\", \"PC1\", \"PC2\", \"PC3\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
